<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-07-11T20:38:36+07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Nicoâ€™s Portfolio</title><subtitle>Nico Renaldo Portfolio Website</subtitle><entry><title type="html">Itin</title><link href="http://localhost:4000/project/itin/" rel="alternate" type="text/html" title="Itin" /><published>2021-07-11T00:00:00+07:00</published><updated>2021-07-11T00:00:00+07:00</updated><id>http://localhost:4000/project/itin</id><content type="html" xml:base="http://localhost:4000/project/itin/">&lt;h2 id=&quot;project-brief&quot;&gt;Project Brief&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;itin&lt;/strong&gt; is a trip planning web application to easily make, share, and browse itineraries.  Users could make their own itinerary, share it with friends and public, and take inspiration from others.
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Link to &lt;a href=&quot;https://itin.netlify.app/&quot;&gt;project demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Itin&lt;/strong&gt; is created using &lt;strong&gt;Flask&lt;/strong&gt; for backend, and &lt;strong&gt;Nuxt.js&lt;/strong&gt; for frontend side. Most people plan their trip using notepad, google search and maps. This old-fashioned way of making itinerary is inflexible, having to switch to many applications and making changes burdensome. Itin provides a way for people to make their itinerary in one place, where user could search destinations, write notes, see suggestions and sharing it to friends.&lt;/p&gt;

&lt;h2 id=&quot;photos&quot;&gt;Photos&lt;/h2&gt;

&lt;div class=&quot;carousel-container&quot;&gt;
      
    &lt;!-- Expanded image --&gt;
    &lt;img id=&quot;expandedImg&quot; src=&quot;/assets/img/project/itin-create.jpg&quot; class=&quot;expanded-img&quot; /&gt;
  
    &lt;!-- Image text --&gt;
    &lt;div id=&quot;imgtext&quot;&gt;Creating Itinerary&lt;/div&gt;
      &lt;!-- Left and right controls --&gt;
  &lt;!--   
    &lt;a class=&quot;prev-button&quot; href=&quot;#demo&quot; data-slide=&quot;prev&quot; onclick=&quot;plusSlides(-1)&quot;&gt;&lt;&lt;/a&gt;
    &lt;a class=&quot;next-button&quot; href=&quot;#demo&quot; data-slide=&quot;next&quot; onclick=&quot;plusSlides(1)&quot;&gt;&gt;&lt;/a&gt; --&gt;
      &lt;!-- &lt;a class=&quot;carousel-control-prev&quot; href=&quot;#demo&quot; data-slide=&quot;prev&quot;&gt;
      &lt;span class=&quot;carousel-control-prev-icon&quot;&gt;&lt;/span&gt;
    &lt;/a&gt;
    &lt;a class=&quot;carousel-control-next&quot; href=&quot;#demo&quot; data-slide=&quot;next&quot;&gt;
      &lt;span class=&quot;carousel-control-next-icon&quot;&gt;&lt;/span&gt;
    &lt;/a&gt; --&gt;
  &lt;/div&gt;

&lt;p&gt;&lt;!-- The grid: four columns --&gt;&lt;/p&gt;
&lt;div class=&quot;carousel&quot;&gt;
      &lt;div class=&quot;items&quot;&gt;
          
          &lt;div class=&quot;item&quot;&gt;
              &lt;img src=&quot;/assets/img/project/itin-create.jpg&quot; alt=&quot;Creating Itinerary&quot; class=&quot;item-img&quot; onclick=&quot;carouselchoose(this);&quot; /&gt;
          &lt;/div&gt;
          
          &lt;div class=&quot;item&quot;&gt;
              &lt;img src=&quot;/assets/img/project/itin-item.png&quot; alt=&quot;Itinerary Details&quot; class=&quot;item-img&quot; onclick=&quot;carouselchoose(this);&quot; /&gt;
          &lt;/div&gt;
          
          &lt;div class=&quot;item&quot;&gt;
              &lt;img src=&quot;/assets/img/project/itin-book.jpg&quot; alt=&quot;Booking Order&quot; class=&quot;item-img&quot; onclick=&quot;carouselchoose(this);&quot; /&gt;
          &lt;/div&gt;
          
          &lt;div class=&quot;item&quot;&gt;
              &lt;img src=&quot;/assets/img/project/itin-browse.jpg&quot; alt=&quot;Browse Itinerary&quot; class=&quot;item-img&quot; onclick=&quot;carouselchoose(this);&quot; /&gt;
          &lt;/div&gt;
          
      &lt;/div&gt;
  &lt;/div&gt;

&lt;h2 id=&quot;introduction-video&quot;&gt;Introduction Video&lt;/h2&gt;
&lt;div class=&quot;ytb-embed-container&quot;&gt;
&lt;iframe height=&quot;480px&quot; width=&quot;854px&quot; class=&quot;ytb-embed&quot; src=&quot;https://www.youtube.com/embed/QpfyA9de6zk&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;</content><author><name></name></author><category term="hardware" /><summary type="html">Project Brief itin is a trip planning web application to easily make, share, and browse itineraries. Users could make their own itinerary, share it with friends and public, and take inspiration from others.</summary></entry><entry><title type="html">Stock Watch</title><link href="http://localhost:4000/project/stock-watch/" rel="alternate" type="text/html" title="Stock Watch" /><published>2021-06-11T00:00:00+07:00</published><updated>2021-06-11T00:00:00+07:00</updated><id>http://localhost:4000/project/stock-watch</id><content type="html" xml:base="http://localhost:4000/project/stock-watch/">&lt;h1 id=&quot;to-be-added&quot;&gt;to be added&lt;/h1&gt;</content><author><name></name></author><category term="hardware" /><summary type="html">to be added</summary></entry><entry><title type="html">Sanyo Inventory System</title><link href="http://localhost:4000/project/sanyo-inventory/" rel="alternate" type="text/html" title="Sanyo Inventory System" /><published>2021-06-11T00:00:00+07:00</published><updated>2021-06-11T00:00:00+07:00</updated><id>http://localhost:4000/project/sanyo-inventory</id><content type="html" xml:base="http://localhost:4000/project/sanyo-inventory/">&lt;h2 id=&quot;project-brief&quot;&gt;Project Brief&lt;/h2&gt;
&lt;p&gt;This project is an inventory management system I created on my holiday. The website is used to track products stock and to track sales and profits. The website is created using using &lt;strong&gt;Django&lt;/strong&gt; and hosted on &lt;strong&gt;Heroku&lt;/strong&gt;. This project teaches me the CRUD basic on Django.
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Link to &lt;a href=&quot;http://sanyo-dashboard.herokuapp.com/&quot;&gt;project demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;photos&quot;&gt;Photos&lt;/h2&gt;

&lt;div class=&quot;carousel-container&quot;&gt;
      
    &lt;!-- Expanded image --&gt;
    &lt;img id=&quot;expandedImg&quot; src=&quot;/assets/img/project/sanyo-chart.png&quot; class=&quot;expanded-img&quot; /&gt;
  
    &lt;!-- Image text --&gt;
    &lt;div id=&quot;imgtext&quot;&gt;Chart of Profit and Stocks&lt;/div&gt;
      &lt;!-- Left and right controls --&gt;
  &lt;!--   
    &lt;a class=&quot;prev-button&quot; href=&quot;#demo&quot; data-slide=&quot;prev&quot; onclick=&quot;plusSlides(-1)&quot;&gt;&lt;&lt;/a&gt;
    &lt;a class=&quot;next-button&quot; href=&quot;#demo&quot; data-slide=&quot;next&quot; onclick=&quot;plusSlides(1)&quot;&gt;&gt;&lt;/a&gt; --&gt;
      &lt;!-- &lt;a class=&quot;carousel-control-prev&quot; href=&quot;#demo&quot; data-slide=&quot;prev&quot;&gt;
      &lt;span class=&quot;carousel-control-prev-icon&quot;&gt;&lt;/span&gt;
    &lt;/a&gt;
    &lt;a class=&quot;carousel-control-next&quot; href=&quot;#demo&quot; data-slide=&quot;next&quot;&gt;
      &lt;span class=&quot;carousel-control-next-icon&quot;&gt;&lt;/span&gt;
    &lt;/a&gt; --&gt;
  &lt;/div&gt;

&lt;p&gt;&lt;!-- The grid: four columns --&gt;&lt;/p&gt;
&lt;div class=&quot;carousel&quot;&gt;
      &lt;div class=&quot;items&quot;&gt;
          
          &lt;div class=&quot;item&quot;&gt;
              &lt;img src=&quot;/assets/img/project/sanyo-chart.png&quot; alt=&quot;Chart of Profit and Stocks&quot; class=&quot;item-img&quot; onclick=&quot;carouselchoose(this);&quot; /&gt;
          &lt;/div&gt;
          
          &lt;div class=&quot;item&quot;&gt;
              &lt;img src=&quot;/assets/img/project/sanyo-home.png&quot; alt=&quot;Browse Page&quot; class=&quot;item-img&quot; onclick=&quot;carouselchoose(this);&quot; /&gt;
          &lt;/div&gt;
          
          &lt;div class=&quot;item&quot;&gt;
              &lt;img src=&quot;/assets/img/project/sanyo-item.png&quot; alt=&quot;Transaction Page&quot; class=&quot;item-img&quot; onclick=&quot;carouselchoose(this);&quot; /&gt;
          &lt;/div&gt;
          
          &lt;div class=&quot;item&quot;&gt;
              &lt;img src=&quot;/assets/img/project/sanyo-transaksi.png&quot; alt=&quot;Transaction Page&quot; class=&quot;item-img&quot; onclick=&quot;carouselchoose(this);&quot; /&gt;
          &lt;/div&gt;
          
      &lt;/div&gt;
  &lt;/div&gt;</content><author><name></name></author><category term="hardware" /><summary type="html">Project Brief This project is an inventory management system I created on my holiday. The website is used to track products stock and to track sales and profits. The website is created using using Django and hosted on Heroku. This project teaches me the CRUD basic on Django.</summary></entry><entry><title type="html">Jagawana</title><link href="http://localhost:4000/project/jagawana/" rel="alternate" type="text/html" title="Jagawana" /><published>2021-06-11T00:00:00+07:00</published><updated>2021-06-11T00:00:00+07:00</updated><id>http://localhost:4000/project/jagawana</id><content type="html" xml:base="http://localhost:4000/project/jagawana/">&lt;h2 id=&quot;project-brief&quot;&gt;Project Brief&lt;/h2&gt;
&lt;p&gt;Jagawana is a wide sensor network system deployed in the forests to prevent &lt;strong&gt;ilegal logging&lt;/strong&gt;. We use sensors to pick up voices in the forests to monitor what happened in the forest in real-time. We use machine learning to process the sounds taken by the sensor and to identify the sounds into various categories, such as &lt;strong&gt;chainsaws, trucks, gunshot, and burning sounds&lt;/strong&gt;. We will be using Android App to monitor and notify the user if suspicious events were happening in the forest.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;
This project combines &lt;strong&gt;machine learning, internet of things, cloud computing, and android application&lt;/strong&gt;. This project is part of &lt;a href=&quot;https://bangkit.academy/&quot;&gt;Bangkit Academyâ€™s Capstone Project&lt;/a&gt;. My role in this project involves creating the Machine Learning Model, designing and developing the Google Cloud Architecture, making the IoT prototype, and creating design and illustration.&lt;/p&gt;

&lt;p&gt;The machine learning model is developed on Kaggle using &lt;a href=&quot;https://www.kaggle.com/mmoreaux/environmental-sound-classification-50&quot;&gt;ESC-50 Audio Dataset&lt;/a&gt;, &lt;a href=&quot;https://www.kaggle.com/chrisfilo/urbansound8k&quot;&gt;Urbansound8k Dataset&lt;/a&gt;, and &lt;a href=&quot;https://research.google.com/audioset/&quot;&gt;Googleâ€™s Audioset&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We are using ESP32 and Mosquitto Broker to prototype the working device. The Google Cloud Platform then will receive and store the audio data using Pub/Sub as a trigger for the cloud functions to store the data to Cloud Storage and BigQuery. Every audio data inputted will be processed by our machine learning model deployed on the AI Platform. You can see the project overview on image below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/project/jagawana-overview.jpg&quot; alt=&quot;Jagawana System Overview&quot; class=&quot;img-responsive&quot; /&gt;
&lt;em&gt;Jagawana System Overview&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;photos&quot;&gt;Photos&lt;/h2&gt;

&lt;div class=&quot;carousel-container&quot;&gt;
      
    &lt;!-- Expanded image --&gt;
    &lt;img id=&quot;expandedImg&quot; src=&quot;/assets/img/project/jagawana-illustration.png&quot; class=&quot;expanded-img&quot; /&gt;
  
    &lt;!-- Image text --&gt;
    &lt;div id=&quot;imgtext&quot;&gt;Illustration&lt;/div&gt;
      &lt;!-- Left and right controls --&gt;
  &lt;!--   
    &lt;a class=&quot;prev-button&quot; href=&quot;#demo&quot; data-slide=&quot;prev&quot; onclick=&quot;plusSlides(-1)&quot;&gt;&lt;&lt;/a&gt;
    &lt;a class=&quot;next-button&quot; href=&quot;#demo&quot; data-slide=&quot;next&quot; onclick=&quot;plusSlides(1)&quot;&gt;&gt;&lt;/a&gt; --&gt;
      &lt;!-- &lt;a class=&quot;carousel-control-prev&quot; href=&quot;#demo&quot; data-slide=&quot;prev&quot;&gt;
      &lt;span class=&quot;carousel-control-prev-icon&quot;&gt;&lt;/span&gt;
    &lt;/a&gt;
    &lt;a class=&quot;carousel-control-next&quot; href=&quot;#demo&quot; data-slide=&quot;next&quot;&gt;
      &lt;span class=&quot;carousel-control-next-icon&quot;&gt;&lt;/span&gt;
    &lt;/a&gt; --&gt;
  &lt;/div&gt;

&lt;p&gt;&lt;!-- The grid: four columns --&gt;&lt;/p&gt;
&lt;div class=&quot;carousel&quot;&gt;
      &lt;div class=&quot;items&quot;&gt;
          
          &lt;div class=&quot;item&quot;&gt;
              &lt;img src=&quot;/assets/img/project/jagawana-illustration.png&quot; alt=&quot;Illustration&quot; class=&quot;item-img&quot; onclick=&quot;carouselchoose(this);&quot; /&gt;
          &lt;/div&gt;
          
          &lt;div class=&quot;item&quot;&gt;
              &lt;img src=&quot;/assets/img/project/featured-jagawana.jpg&quot; alt=&quot;Deployed System&quot; class=&quot;item-img&quot; onclick=&quot;carouselchoose(this);&quot; /&gt;
          &lt;/div&gt;
          
          &lt;div class=&quot;item&quot;&gt;
              &lt;img src=&quot;/assets/img/project/jagawana-comparison.jpg&quot; alt=&quot;Sound Comparison&quot; class=&quot;item-img&quot; onclick=&quot;carouselchoose(this);&quot; /&gt;
          &lt;/div&gt;
          
          &lt;div class=&quot;item&quot;&gt;
              &lt;img src=&quot;/assets/img/project/jagawana-device.png&quot; alt=&quot;IoT Device&quot; class=&quot;item-img&quot; onclick=&quot;carouselchoose(this);&quot; /&gt;
          &lt;/div&gt;
          
      &lt;/div&gt;
  &lt;/div&gt;

&lt;h2 id=&quot;introduction-video&quot;&gt;Introduction Video&lt;/h2&gt;
&lt;div class=&quot;ytb-embed-container&quot;&gt;
&lt;iframe height=&quot;480px&quot; width=&quot;854px&quot; class=&quot;ytb-embed&quot; src=&quot;https://www.youtube.com/embed/nLUPU1pUyE0&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&quot;related-links&quot;&gt;Related Links&lt;/h2&gt;
&lt;p&gt;I made a medium stories explaining the project in details. The github repo link can be seen on the About Jagawana stories.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://nicorenaldo.medium.com/detecting-chainsaws-in-forest-with-machine-learning-jagawana-989fd345784&quot;&gt;About Jagawana&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://nicorenaldo.medium.com/jagawana-machine-learning-in-depth-6ea66a45d6b2&quot;&gt;ML in Depth&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://nicorenaldo.medium.com/jagawana-internet-of-things-511d63e48349&quot;&gt;IoT in Depth&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="hardware" /><summary type="html">Project Brief Jagawana is a wide sensor network system deployed in the forests to prevent ilegal logging. We use sensors to pick up voices in the forests to monitor what happened in the forest in real-time. We use machine learning to process the sounds taken by the sensor and to identify the sounds into various categories, such as chainsaws, trucks, gunshot, and burning sounds. We will be using Android App to monitor and notify the user if suspicious events were happening in the forest.</summary></entry><entry><title type="html">AyoLomba!</title><link href="http://localhost:4000/project/ayolomba/" rel="alternate" type="text/html" title="AyoLomba!" /><published>2021-06-11T00:00:00+07:00</published><updated>2021-06-11T00:00:00+07:00</updated><id>http://localhost:4000/project/ayolomba</id><content type="html" xml:base="http://localhost:4000/project/ayolomba/">&lt;h2 id=&quot;project-brief&quot;&gt;Project Brief&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;AyoLomba!&lt;/strong&gt; is a web platform to accomodate event organizers and students across Indonesia to post and search competitions, scholarships, and webinars with better experience. Members can find the event they want based on their preferences, AyoLomba! also helps out organizers to reach their audience, using sleek UI and many features to help out both parties.
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Link to &lt;a href=&quot;https://ayolomba-id.herokuapp.com/&quot;&gt;project demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;AyoLomba!&lt;/strong&gt; is created using &lt;strong&gt;Django&lt;/strong&gt; and hosted on &lt;strong&gt;Heroku&lt;/strong&gt;. While there are many option to find information about events, most of them rely on social media like Instagram and Line that doesnâ€™t have any filtering feature. This is why we are motivated to create a dedicated platform using web and android app, that solve this irritable issue. This project is made prior to the emerging giant &lt;strong&gt;Sejuta Cita&lt;/strong&gt; which holds the same concept with us.&lt;/p&gt;

&lt;h2 id=&quot;photos&quot;&gt;Photos&lt;/h2&gt;

&lt;div class=&quot;carousel-container&quot;&gt;
      
    &lt;!-- Expanded image --&gt;
    &lt;img id=&quot;expandedImg&quot; src=&quot;/assets/img/project/ayolomba-form.jpg&quot; class=&quot;expanded-img&quot; /&gt;
  
    &lt;!-- Image text --&gt;
    &lt;div id=&quot;imgtext&quot;&gt;Posting Form&lt;/div&gt;
      &lt;!-- Left and right controls --&gt;
  &lt;!--   
    &lt;a class=&quot;prev-button&quot; href=&quot;#demo&quot; data-slide=&quot;prev&quot; onclick=&quot;plusSlides(-1)&quot;&gt;&lt;&lt;/a&gt;
    &lt;a class=&quot;next-button&quot; href=&quot;#demo&quot; data-slide=&quot;next&quot; onclick=&quot;plusSlides(1)&quot;&gt;&gt;&lt;/a&gt; --&gt;
      &lt;!-- &lt;a class=&quot;carousel-control-prev&quot; href=&quot;#demo&quot; data-slide=&quot;prev&quot;&gt;
      &lt;span class=&quot;carousel-control-prev-icon&quot;&gt;&lt;/span&gt;
    &lt;/a&gt;
    &lt;a class=&quot;carousel-control-next&quot; href=&quot;#demo&quot; data-slide=&quot;next&quot;&gt;
      &lt;span class=&quot;carousel-control-next-icon&quot;&gt;&lt;/span&gt;
    &lt;/a&gt; --&gt;
  &lt;/div&gt;

&lt;p&gt;&lt;!-- The grid: four columns --&gt;&lt;/p&gt;
&lt;div class=&quot;carousel&quot;&gt;
      &lt;div class=&quot;items&quot;&gt;
          
          &lt;div class=&quot;item&quot;&gt;
              &lt;img src=&quot;/assets/img/project/ayolomba-form.jpg&quot; alt=&quot;Posting Form&quot; class=&quot;item-img&quot; onclick=&quot;carouselchoose(this);&quot; /&gt;
          &lt;/div&gt;
          
          &lt;div class=&quot;item&quot;&gt;
              &lt;img src=&quot;/assets/img/project/ayolomba-search2.jpg&quot; alt=&quot;Browse Page&quot; class=&quot;item-img&quot; onclick=&quot;carouselchoose(this);&quot; /&gt;
          &lt;/div&gt;
          
          &lt;div class=&quot;item&quot;&gt;
              &lt;img src=&quot;/assets/img/project/ayolomba-lomba.jpg&quot; alt=&quot;Event Details&quot; class=&quot;item-img&quot; onclick=&quot;carouselchoose(this);&quot; /&gt;
          &lt;/div&gt;
          
          &lt;div class=&quot;item&quot;&gt;
              &lt;img src=&quot;/assets/img/project/ayolomba-about.jpg&quot; alt=&quot;About Us&quot; class=&quot;item-img&quot; onclick=&quot;carouselchoose(this);&quot; /&gt;
          &lt;/div&gt;
          
      &lt;/div&gt;
  &lt;/div&gt;</content><author><name></name></author><category term="hardware" /><summary type="html">Project Brief AyoLomba! is a web platform to accomodate event organizers and students across Indonesia to post and search competitions, scholarships, and webinars with better experience. Members can find the event they want based on their preferences, AyoLomba! also helps out organizers to reach their audience, using sleek UI and many features to help out both parties.</summary></entry><entry><title type="html">Jagawana â€” Machine Learning In Depth</title><link href="http://localhost:4000/blog/jagawana-machine-learning/" rel="alternate" type="text/html" title="Jagawana â€” Machine Learning In Depth" /><published>2021-06-10T00:00:00+07:00</published><updated>2021-06-10T00:00:00+07:00</updated><id>http://localhost:4000/blog/jagawana-machine-learning</id><content type="html" xml:base="http://localhost:4000/blog/jagawana-machine-learning/">&lt;p&gt;&lt;img src=&quot;/assets/img/blogs/soundwave.png&quot; alt=&quot;Soundwave&quot; class=&quot;img-responsive&quot; /&gt;&lt;/p&gt;

&lt;p class=&quot;quote&quot;&gt;
    Jagawana is a Wide Sensor Network System deployed in the forests to prevent Ilegal Logging. By using sensors to pick up voices in the forests, we could monitor what happened in the forest in real-time. We deployed a Machine Learning Model to process the sounds taken by the sensor, then the model will identify the sounds into various categories, such as chainsaws, trucks, gunshot, and burning sounds. We will be using Android App to monitor and notify the user if suspicious events were happening in the forest, the user could also be able to hear the sound itself to ensure the results are correct. &lt;a href=&quot;http://localhost:4000/blog/jagawana-overview/&quot;&gt;Link to the project overview&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;Our Machine Learning Model main goal is to Classify Forests Ambience Sounds taken by the sensors. Our priority is to identify chainsaw sounds and alert users from Android App. Though identifying other sounds is as important too. Being able to identify other sounds may enable us to map out fauna habitats, and for further research data.&lt;/p&gt;

&lt;div class=&quot;separator&quot; role=&quot;separator&quot;&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;
&lt;h3 id=&quot;where-to-develop&quot;&gt;Where to Develop&lt;/h3&gt;
&lt;p&gt;We developed our model using Kaggle Notebook, you can check it on this &lt;a href=&quot;https://www.kaggle.com/nicorenaldo/jagawana-v2-forest-logging-detection&quot;&gt;link&lt;/a&gt;. Developing the model in the Kaggle is really helpful since the dataset is saved on Kaggle Server, so we donâ€™t need to download it over and over again like in Google Colab.&lt;/p&gt;

&lt;h3 id=&quot;datasets&quot;&gt;Datasets&lt;/h3&gt;
&lt;p&gt;The dataset we are using are :&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/mmoreaux/environmental-sound-classification-50&quot;&gt;ESC-50&lt;/a&gt; : Chainsaw and crackling fire sounds&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/chrisfilo/urbansound8k&quot;&gt;Urbansound8k&lt;/a&gt; : Gunshot sounds&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://research.google.com/audioset/&quot;&gt;Googleâ€™s Audioset&lt;/a&gt; : Chainsaw, crackling fire, and gunshot sounds&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Googleâ€™s Audioset data consists of CSV data with youtube links, intervals, and categories. To actually get the audio data you need to do some more work, I use the script from &lt;a href=&quot;https://github.com/aoifemcdonagh/audioset-processing&quot;&gt;here&lt;/a&gt; which I modified to add a download limiter, you can check the my modificated repo &lt;a href=&quot;https://github.com/nicorenaldo/audioset-processing&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;paper-references&quot;&gt;Paper References&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1908.07517&quot;&gt;AI for Earth: Rainforest Conservation by Acoustic Surveillance&lt;/a&gt;
&lt;a href=&quot;https://arxiv.org/abs/1810.09078&quot;&gt;Our Practice Of Using Machine Learning To Recognize Species By Voice&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;separator&quot; role=&quot;separator&quot;&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;h2 id=&quot;general-approach&quot;&gt;General Approach&lt;/h2&gt;
&lt;p&gt;There are many image recognition examples in online courses and tutorials. The way machine learning classifies an image is by analyzing the pixel value as a series. Based on this analogy, we should be able to analyze sounds based on the amplitude as a time series, but this is actually not the case.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blogs/soundwave-mel.jpeg&quot; alt=&quot;Soundwave&quot; class=&quot;img-responsive&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A waveform is an amplitude measurement put in a time series. There are many disadvantages to analyze the waveform, the data is big and messy, and the machine learning model doesnâ€™t perform well. The way around it is by analyzing the audio data and turn it into features, by analyzing it on the frequency domain (rather than amplitude) using math. In our experiment, we are using the &lt;a href=&quot;https://towardsdatascience.com/getting-to-know-the-mel-spectrogram-31bca3e2d9d0&quot;&gt;Mel Spectrogram&lt;/a&gt; feature, this feature basically summarizes the waveform into a set of coefficients used in Mel Spectrogram and can be plotted like the image above. Using Mel Spectrogram which has less size than the actual waveform, we can train our model faster and get better performance too.&lt;/p&gt;

&lt;p&gt;There are other features that we can extract from a waveform, like normal Spectrogram, MFCC, and many else. Here is a reference you can use to learn more about &lt;a href=&quot;https://towardsdatascience.com/audio-deep-learning-made-simple-part-1-state-of-the-art-techniques-da1d3dff2504&quot;&gt;what is sound and how it is digitized&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;machine-learning-model&quot;&gt;Machine Learning Model&lt;/h2&gt;
&lt;p&gt;The most common approach use features like Mel Spectrogram and MFCC, but how about the model? Unexpectedly, the common CNN model used on image classification actually performs well on this data and has been a popularly used approach to classify audio.&lt;/p&gt;

&lt;p&gt;In this project, we use the VGG16 model as our baseline. &lt;a href=&quot;https://neurohive.io/en/popular-networks/vgg16/&quot;&gt;VGG16&lt;/a&gt; is a popular CNN model used on image classification, the model achieves 92.7% top-5 test accuracy in ImageNet, which is a dataset of over 14 million images belonging to 1000 classes. VGG16 is consisted of 16 layers, with over 140M parameters and a size of 533MB. This makes training and deploying the model a tiresome task.&lt;/p&gt;

&lt;p&gt;Our team did a modification to the model quite similar to &lt;a href=&quot;https://arxiv.org/abs/1908.07517&quot;&gt;this paper&lt;/a&gt;. Our augmented model has only 1M parameters, with a size of 15MB. The modification on the paper made are as follows :&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Adding a Batch Normalization layer following each convolutional layer. Batch Normalization allows much higher learning rates and is less sensitive to initialization.&lt;/li&gt;
  &lt;li&gt;A global pooling layer replaces a flattened layer. Adding the global pooling layer not only helps filter the features but also makes the network adapt to different sizes of input spectrograms.&lt;/li&gt;
  &lt;li&gt;The 4096-unit FC layers are reduced to 256-unit FC layers.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Along with our modification as follows :&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Adding dropout layer following each convolutional block.&lt;/li&gt;
  &lt;li&gt;Reducing the convolutional block from 4 to 3 blocks.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Our final model can be seen &lt;a href=&quot;https://postimg.cc/RJ7wJjwT&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;preprocessing-and-training&quot;&gt;Preprocessing and Training&lt;/h2&gt;
&lt;p&gt;After the model, there comes the preprocessing. The data we are using comes from 3 different sources and they have different characteristics too.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;ESC-50; we are using the chainsaw and crackling fire sound from this dataset, all the audio data is 5 seconds long, with only 40 recordings for each class.&lt;/li&gt;
  &lt;li&gt;Urbansound5k; we are using the gunshot sound from this dataset, there are 374 recordings and the length varies from 1 to 4 seconds.&lt;/li&gt;
  &lt;li&gt;Google Audioset; all 3 classes exist here, the dataset itself is a list of associated YouTube ID, start time, end time, and class labels in a CSV file. To use the dataset as audio data, we need to download the recordings from youtube using a script that I have modified &lt;a href=&quot;https://github.com/nicorenaldo/audioset-processing&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To train the model, we need to have the same dimension of audio data, since we have various length of data, we decided to use only 2 seconds of recordings at a time. To get such data, we have to do some preprocessing.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;ESC-50, we use a shifting windowing to trim 4 clips of 2 seconds from the original clip, totaling 40x4 = 160 clips for the 2 classes.&lt;/li&gt;
  &lt;li&gt;Urbansound5k, we are padding the recording with a length less than 2 seconds, and trimming the one with a length of more than 2 seconds.&lt;/li&gt;
  &lt;li&gt;The script to extract audio from youtube will download 10 seconds recordings. Using shifting windowing we will get 9 clips of 2 seconds from the original clip.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In our experiment, we are using 320 clips for each classes. The data which is still in the waveform then will be processed into Mel Spectrogram and shuffled into the train and validation dataset.&lt;/p&gt;

&lt;p&gt;Training our model from scratch, the model got stopped by the callback after 15 epochs with results below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blogs/soundwave-training.jpg&quot; alt=&quot;Training History&quot; class=&quot;img-responsive&quot; /&gt;
&lt;em&gt;Accuracy &amp;amp; Loss History and Confusion Matrix&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;From the confusion matrix, we could see the model perform quite nicely for all 3 classes.&lt;/p&gt;

&lt;h2 id=&quot;inference--predicting&quot;&gt;Inference / Predicting&lt;/h2&gt;
&lt;p&gt;We are using 2 seconds audio to train our model, but how do we analyze audio that has a different length or even through streaming? You could actually pass the whole audio data to the model, that is because the model could use any input size, but this method will perform worse than other methods. Another way to do it is by predicting a window of n seconds audio, and shifting it 1 second at a time, this method really works well but comes at a price, the computational needed will be huge.&lt;/p&gt;

&lt;p&gt;What our team proposed is by reducing the noise from an audio clip, we could separate/highlight/isolate the target sounds that we want to hear. The target sounds will have a set of intervals, and the model will predict each highlighted clip separately. You can see the illustration of the method in the image below. After reducing the noise, the output results into 5 highlighted clips.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blogs/soundwave-clip.png&quot; alt=&quot;Soundwave Clipping&quot; class=&quot;img-responsive&quot; /&gt;
&lt;em&gt;Before and After Highlighting the Sounds&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;There are still many aspects that could be improved, especially researching different model architecture, using a better dataset, and using a different audio feature like MFCC. Feel free to ask questions and using my Kaggle notebook as a reference, I hope my work on this project could help someone else in researching this topic.&lt;/p&gt;</content><author><name></name></author><category term="hardware" /><category term="Bangkit" /><category term="Jagawana" /><category term="Audio Classification" /><category term="Chainsaws" /><category term="Illegal Logging" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/soundwave.png" /><media:content medium="image" url="http://localhost:4000/soundwave.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Jagawana â€” Internet of Things</title><link href="http://localhost:4000/blog/jagawana-iot/" rel="alternate" type="text/html" title="Jagawana â€” Internet of Things" /><published>2021-06-05T00:00:00+07:00</published><updated>2021-06-05T00:00:00+07:00</updated><id>http://localhost:4000/blog/jagawana-iot</id><content type="html" xml:base="http://localhost:4000/blog/jagawana-iot/">&lt;p&gt;&lt;img src=&quot;/assets/img/project/featured-jagawana.jpg&quot; alt=&quot;Soundwave&quot; class=&quot;img-responsive&quot; /&gt;&lt;/p&gt;

&lt;p class=&quot;quote&quot;&gt;
    Jagawana is a Wide Sensor Network System deployed in the forests to prevent Ilegal Logging. By using sensors to pick up voices in the forests, we could monitor what happened in the forest in real-time. We deployed a Machine Learning Model to process the sounds taken by the sensor, then the model will identify the sounds into various categories, such as chainsaws, trucks, gunshot, and burning sounds. We will be using Android App to monitor and notify the user if suspicious events were happening in the forest, the user could also be able to hear the sound itself to ensure the results are correct. &lt;a href=&quot;http://localhost:4000/blog/jagawana-overview/&quot;&gt;Link to the project overview&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;One of the keys to our project is how we will gather audio from the forest, our solution involves creating a Wide Sensor Network that spans across the forest. This idea is popularly used by many researchers, especially in detecting forest fires using a smoke detector.&lt;/p&gt;

&lt;p&gt;Our idea is to deploy the sensor device along with microphones, battery, solar panel, SD card, and an RF transmitter. Ideally, the sensor device would be able to detect and record sounds, have a long long battery life, and communicate far away using an RF transmitter. This may sound complex and overwhelming, so many engineering fields are involved from programming, electronics, and also telemetry. Thus, we are trying to keep it simple and not go into the details soon.&lt;/p&gt;

&lt;div class=&quot;separator&quot; role=&quot;separator&quot;&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;h3 id=&quot;the-device&quot;&gt;The Device&lt;/h3&gt;
&lt;p&gt;The device itself has several specifications, first off it must have enough flash memory to transmit data and do simple filtering for sounds. The general option would be ESP32 and Teensy, both are powerful and cheap prototype microcontrollers and popularly used by many hobbyists. There is also the alternative to using Raspberry Pi, but the costs of material will jump out too much.&lt;/p&gt;

&lt;p&gt;The device will be deployed in a remote location, which means the device needs to be sustainable. Our solution is by using a solar panel and a battery. We havenâ€™t started our research on this, so I will skip the details.&lt;/p&gt;

&lt;h3 id=&quot;the-sounds&quot;&gt;The Sounds&lt;/h3&gt;
&lt;p&gt;Our device will be the ears, listening to every sound inside the forest. We havenâ€™t found a suitable microphone module for the job, as the microphone is supposed to hear from distances (~100m â€” 1000m) to be efficient and cutting down the number of devices needed. Our prototyping model uses a generic microphone module such as MAX9814 (Analog) or INMP44 (I2S).&lt;/p&gt;

&lt;p&gt;To prevent overloading the memory and transmission, we need a mechanism to prevent any sounds to trigger the device. We are proposing the idea to use the same mechanism as Amazonâ€™s Alexa and other home devices. Alexa didnâ€™t send any audio data to the cloud until the wake word is called. The way it works is by having Alexa listening all the time, storing a short audio data into the buffer memory, process it whether the wake word is called or not, if not then Alexa will record a short audio data again replacing the buffer memory. Using this mechanism, Alexa wonâ€™t run out of data even if it listens to our conversation all day long.&lt;/p&gt;

&lt;p&gt;The general idea would look like the flowchart below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blogs/iot-flowchart.png&quot; alt=&quot;Diagram Flow of Passive Listening&quot; class=&quot;img-responsive&quot; /&gt;
&lt;em&gt;Diagram Flow of Passive Listening&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Our first plan is to use a simple CNN model fitted into our device, we havenâ€™t researched that area yet so we proposed a plan B which is by analyzing the audio data and use the chainsaw frequency as the trigger. Chainsaw sounds dominate in frequency bands between 500 Hz and 8 kHz, by extracting the audio data into Mel Spectrogram, we could see the uniqueness of chainsaw sounds.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/blogs/iot-comparison.jpg&quot; alt=&quot;Comparison of Different Sounds&quot; class=&quot;img-responsive&quot; /&gt;
&lt;em&gt;Comparison of Different Sounds&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-network&quot;&gt;The Network&lt;/h3&gt;
&lt;p&gt;This is the hardest part of the system. In real-life situations, especially in rural areas like forests, there will be no cellular signal at all. To communicate with each other, we are going to use an RF transmitter especially looking at the LoRaWAN module. LoRa is communication technology, just like WiFi and Bluetooth, that specializes in long-range and low-power communication. Using LoRa, devices could communicate up to 2 kilometers through obstacles, and up to 20 km with a line of sight.&lt;/p&gt;

&lt;p&gt;Creating a sensor network is not as simple as that, to cover a whole area with as few sensors as possible, there needs to be a network architecture that is effective and robust. Again, this topic is really heavy and discussed in many papers like &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0198971512000300&quot;&gt;this&lt;/a&gt;, &lt;a href=&quot;https://www.researchgate.net/figure/The-proposed-architecture-for-forest-fire-detection_fig1_305773926&quot;&gt;this&lt;/a&gt;, and &lt;a href=&quot;https://ieeexplore.ieee.org/document/5691483&quot;&gt;this one&lt;/a&gt;. We proposed a simple model to illustrate and simplify the detail, but the real-life deployment will need a more complex model to effectively be used.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/project/featured-jagawana.jpg&quot; alt=&quot;Simple Illustration of Sensor Network
&quot; class=&quot;img-responsive&quot; /&gt;
&lt;em&gt;Simple Illustration of Sensor Network&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;separator&quot; role=&quot;separator&quot;&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;h2 id=&quot;current-progress&quot;&gt;Current Progress&lt;/h2&gt;
&lt;p&gt;Limited by time and budget, on this 1 month project we decided to only develop our device in the home environment, using ESP32, MAX9814 microphone, and an SD card reader.&lt;/p&gt;

&lt;p&gt;The device would communicate with mosquitto broker on my laptop using MQTT, and the laptop as the gateway will forward the audio data to Google Cloud Platform for further processing. I have documented the code on my Github repo here.&lt;/p&gt;

&lt;p&gt;The requirement I set for our prototype board is :&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Record and Store audio data to .wav format in SD card&lt;/li&gt;
  &lt;li&gt;Read the data and send it using MQTT&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;separator&quot; role=&quot;separator&quot;&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;h3 id=&quot;record-and-store-audio-data&quot;&gt;Record and Store Audio Data&lt;/h3&gt;
&lt;p&gt;First off, there are two kinds of microphone modules out there, the analog one like MAX9814, and the one with I2S support like INMP441. I bought the MAX9814 without doing research beforehand, but I would recommend you guys to try the one with I2S support.&lt;/p&gt;

&lt;p&gt;Recording audio data is a challenge itself, but to store the audio data has a trick on itâ€™s own. There is a thing called &lt;a href=&quot;https://www.embedded.com/introduction-to-direct-memory-access/&quot;&gt;DMA&lt;/a&gt; which stands for Direct Memory Access, this will allow us to write the audio data directly to memory without any interference from the CPU. This will allow our program and CPU not to freeze due to saving data to memory. I was originally to program the ESP32 using Micropython, but the &lt;a href=&quot;https://github.com/miketeachman/micropython-esp32-i2s-examples&quot;&gt;library&lt;/a&gt; I found only supports DMA and I2S, so me and my analog microphone canâ€™t use the library. After gone through testing and searching, I found this &lt;a href=&quot;https://github.com/MhageGH/esp32_SoundRecorder/&quot;&gt;C++ library&lt;/a&gt; that do support analog microphone for DMA, and so I modified the code to communicate with my MQTT broker.&lt;/p&gt;

&lt;p&gt;The code on the repo itself is quite explanatory, it has several configuration code to adjust the parameter like duration and sampling rate. Iâ€™m trying to adjust it to my dataset which is using 8-bit resolution and 16KHz sampling rate but the audio file got distorted, Iâ€™m still trying to figure out the solution.&lt;/p&gt;

&lt;h3 id=&quot;sending-data-to-mqtt&quot;&gt;Sending Data to MQTT&lt;/h3&gt;
&lt;p&gt;MQTT is a lightweight communication protocol, popularly used in microcontrollers. Here is a great &lt;a href=&quot;https://medium.com/@onur.dundar1/mqtt-part-i-understanding-mqtt-aade455baec9&quot;&gt;article about MQTT&lt;/a&gt;. Using C++ and Arduino IDE, Iâ€™m using the most popular MQTT library out there from &lt;a href=&quot;https://github.com/knolleary/pubsubclient&quot;&gt;PubSubClient&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Although itâ€™s not intended to be, Iâ€™m going to send my audio data through MQTT for the sake of fast prototyping. Through my experiment, a 10-second audio data (44100Hz Sampling Rate) takes about 800 KB of memory. The PubSubClient has a maximum packet size of 256 bytes by default, there is just no way I could send the full data in one go. What I had to do is to read and send the audio data chunk by chunk, the data will go to a buffer variable and replaced by a new chunk and another.&lt;/p&gt;

&lt;p&gt;I first started reading and sending a chunk of 64 bytes, the whole file takes 147-second with a total of 13782 packets sent. It takes too much time since the buffer size is too small, I ended up editing the libraryâ€™s maximum packet size, and change the buffer size to 512 bytes. The file is successfully sent in 23-second with a total of 1723 sent.&lt;/p&gt;

&lt;p&gt;To receive the data properly on the other side, I had the ESP32 send status using MQTT to begin receiving and saving the data to a file. You can check the code on my repo link below.&lt;/p&gt;

&lt;div class=&quot;separator&quot; role=&quot;separator&quot;&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;h2 id=&quot;last-words&quot;&gt;Last Words&lt;/h2&gt;
&lt;p&gt;The prototype is still in the early stages, there are many things to improve or even hasnâ€™t started yet. This project really challenges me since I havenâ€™t worked with audio data before on a microcontroller. Formatting and saving data is a big challenge. Transmitting the audio data through WiFi is already troublesome enough, but doing the same thing on LoRa network will be even more challenging.&lt;/p&gt;

&lt;p&gt;Please give suggestions if you have any tips to solve the unending problems on this project. I hope I can see the end of this project. Feel free to ask anything if you want to replicate this project.&lt;/p&gt;

&lt;div class=&quot;separator&quot; role=&quot;separator&quot;&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/nicorenaldo/jagawana-iot&quot;&gt;My Code on Github&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/MhageGH/esp32_SoundRecorder/&quot;&gt;C++ Analog Microphone Library&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="hardware" /><category term="Bangkit" /><category term="Jagawana" /><category term="Esp32 Audio" /><category term="Record Audio" /><category term="Mqtt Audio" /><category term="record audio" /><category term="send audio mqtt" /><category term="esp32 audio" /><category term="microphone arduino" /><category term="jagawana" /><category term="bangkit 2021" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/project/featured-jagawana.jpg" /><media:content medium="image" url="http://localhost:4000/project/featured-jagawana.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Detecting Chainsaws in Forest with Machine Learning â€” Jagawana</title><link href="http://localhost:4000/blog/jagawana-overview/" rel="alternate" type="text/html" title="Detecting Chainsaws in Forest with Machine Learning â€” Jagawana" /><published>2021-05-16T00:00:00+07:00</published><updated>2021-05-16T00:00:00+07:00</updated><id>http://localhost:4000/blog/jagawana-overview</id><content type="html" xml:base="http://localhost:4000/blog/jagawana-overview/">&lt;p&gt;In the last post, I talked about my experience at Bangkit Academy 2021 as a Machine Learning Path Participant. After completing the assigned course, we were given a task to do a capstone project by solving a real-life project using our knowledge on Machine Learning, Android Development, and Cloud Computing.&lt;/p&gt;

&lt;p&gt;After forming our group (2 members from each learning path), we start brainstorming our ideas, looking over various themes and different problems, and eventually, we decided to tackle the problem of illegal logging through Indonesiaâ€™s forest. While it may not have huge business potential like Bangkit intended, we still think this idea is unique and interesting to tackle and could be implemented in this very nation.&lt;/p&gt;

&lt;div class=&quot;separator&quot; role=&quot;separator&quot;&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;h2 id=&quot;problems-and-solutions&quot;&gt;Problems and Solutions&lt;/h2&gt;
&lt;p&gt;Forests are huge and the terrain is hard to pass through, on the other side, forest ranger usually comprises of only several people. Often, rangers are patrolling the forest area for 1â€“2 weeks in a month, which means there are many opportunities for illegal loggers to get in and out without any patrol. This gap hole could be prevented by incorporating technology for the ranger and forests.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Jagawana&lt;/strong&gt; is a Wide Sensor Network System deployed in the forests to prevent Ilegal Logging. By using sensors to pick up voices in the forests, we could monitor what happened in the forest in real-time. We deployed a Machine Learning Model to process the sounds taken by the sensor, then the model will identify the sounds into various categories, such as chainsaws, trucks, gunshot, and burning sounds. We will be using Android App to monitor and notify the user if suspicious events were happening in the forest, the user could also be able to hear the sound itself to ensure the results are correct.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/project/jagawana-illustration.png&quot; alt=&quot;Jagawana System Overview&quot; class=&quot;img-responsive&quot; /&gt;
&lt;em&gt;Jagawana System Overview&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;separator&quot; role=&quot;separator&quot;&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;h2 id=&quot;system-overview&quot;&gt;System Overview&lt;/h2&gt;
&lt;p&gt;There are four key components in this project, from &lt;strong&gt;Wide Sensor Network&lt;/strong&gt; (WSN) as our ears, &lt;strong&gt;Machine Learning&lt;/strong&gt;, &lt;strong&gt;Google Cloud Platform&lt;/strong&gt; to deploy our model and receive messages from sensors, and &lt;strong&gt;Android App&lt;/strong&gt; to access data and receive notification. I will explain the system design and some considerations we made during the project.&lt;/p&gt;

&lt;h3 id=&quot;1-wide-sensor-network&quot;&gt;1. Wide Sensor Network&lt;/h3&gt;
&lt;p&gt;As the ears of our project, the sensor network is going to be deployed on the forest, complete with a microphone, microcontroller, SD Card, RF transmitter, battery, and solar panels. There are many papers discussing the deployment system of the WSN, but our project is really limited to time and costs, so what we are going to do is making a prototype on our home using ESP32 and a Microphone.
If you are familiar with the concept of IoT, the ESP32 will be our sensor nodes and we are using a Laptop with Mosquitto Broker as our gateway. The ESP32 will send the audio data to the gateway, where the audio data will be forwarded to our cloud API. Check the details on this post&lt;/p&gt;

&lt;div class=&quot;carousel&quot;&gt;
&lt;div class=&quot;items&quot;&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/img/project/featured-jagawana.jpg&quot; alt=&quot;Wide Sensor Network Illustration&quot; class=&quot;img-responsive&quot; /&gt;
&lt;em&gt;Wide Sensor Network Illustration&lt;/em&gt;&lt;/p&gt;

  &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&quot;2-google-cloud-platform&quot;&gt;2. Google Cloud Platform&lt;/h3&gt;
&lt;p&gt;To connect all three parts of our project, we are going to use Google Cloud Platform (GCP) to receive payloads from Sensor Nodes, deploying ML model and predicting sounds, managing databases, and creating an API for Android App. We use our free trial budget for this project, so that means you could replicate our works for free too.
The service we are using for the sensor networks are IoT Core for registering device and Pub/Sub service for receiving the data through MQTT. To collect the data received, we are using a Cloud Functions service with Pub/Sub message as the trigger. The cloud function when triggered will save the received message to BigQuery and Cloud Storage. We use the AI Platform service to deploy our model for online prediction. We use a Flask server hosted on GCP App Engine as an API for the Android App to access the database. I will explain the services we use in detail later in a separate (tba).&lt;/p&gt;

&lt;h3 id=&quot;3-machine-learning-model&quot;&gt;3. Machine Learning Model&lt;/h3&gt;
&lt;p&gt;Our machine learning model task is to classify sounds into various categories, the number of categories it could predict depends on the dataset it is used to train to. Due to limitations of time and data, currently, we are only classifying chainsaws, crackling fire, and gunshot sounds. Iâ€™m using Kaggle to train the model, where the model will be deployed in GCP. More details on the journey of developing the Machine Learning Model here.&lt;/p&gt;

&lt;h3 id=&quot;4-android-application&quot;&gt;4. Android Application&lt;/h3&gt;
&lt;p&gt;The Android App will be our last checkpoint, we are going to access the data stored and processed by GCP using Android App. The appâ€™s main tasks are to notify the user if there were any suspicious activity, giving maps of location, and playing the suspicious audio detected.&lt;/p&gt;

&lt;div class=&quot;separator&quot; role=&quot;separator&quot;&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;h4 id=&quot;our-overall-project-would-look-like-the-image-below&quot;&gt;Our overall project would look like the image below.&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/img/project/jagawana-overview.jpg&quot; alt=&quot;Project Design Overview&quot; class=&quot;img-responsive&quot; /&gt;
&lt;em&gt;Project Design Overview&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;separator&quot; role=&quot;separator&quot;&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;h4 id=&quot;here-is-a-simple-demo-of-our-project&quot;&gt;Here is a simple demo of our project&lt;/h4&gt;
&lt;div class=&quot;ytb-embed-container&quot;&gt;
&lt;iframe height=&quot;480px&quot; width=&quot;854px&quot; class=&quot;ytb-embed&quot; src=&quot;https://www.youtube.com/embed/nLUPU1pUyE0&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;div class=&quot;separator&quot; role=&quot;separator&quot;&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;h2 id=&quot;end-ofwords&quot;&gt;End ofÂ Words&lt;/h2&gt;
&lt;p&gt;We are working towards completing our MVP product, but there are still many improvements and features that we could add to the project. Some of the features that we didnâ€™t include is using ML to map out animalâ€™s habitat using their sounds and detecting human voices. I hope this article would inspire others to take the research further, feel free to ask/discuss in the comment section.&lt;/p&gt;

&lt;div class=&quot;separator&quot; role=&quot;separator&quot;&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
    &lt;span class=&quot;dot&quot;&gt;&lt;/span&gt;
&lt;/div&gt;
&lt;h4 id=&quot;related-link&quot;&gt;Related Link&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;/blog/jagawana-iot/&quot;&gt;JagawanaÂ : Internet of Things&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;/blog/jagawana-machine-learning/&quot;&gt;JagawanaÂ : Machine Learning in Depth&lt;/a&gt;&lt;br /&gt;
Cloud Computing Post â€” to be added&lt;br /&gt;
Android App Post â€” to be added&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;github-repos&quot;&gt;Github Repos&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/nicorenaldo/jagawana-iot&quot;&gt;Internet of Things&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/nicorenaldo/jagawana-ml&quot;&gt;Machine Learning&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/jeffrywu28/jagawana-cloud&quot;&gt;Cloud Computing&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://github.com/Bobby-Anggunawan/Jagawana-AndroidApp&quot;&gt;Android App&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name></name></author><category term="hardware" /><category term="classify audio" /><category term="predict audio" /><category term="ML audio" /><category term="detect voice" /><category term="forest fire" /><category term="capstone project" /><category term="bangkit capstone" /><category term="forest protect" /><category term="jagawana" /><summary type="html">In the last post, I talked about my experience at Bangkit Academy 2021 as a Machine Learning Path Participant. After completing the assigned course, we were given a task to do a capstone project by solving a real-life project using our knowledge on Machine Learning, Android Development, and Cloud Computing.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/project/jagawana-illustration.png" /><media:content medium="image" url="http://localhost:4000/project/jagawana-illustration.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>